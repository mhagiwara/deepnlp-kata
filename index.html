<!DOCTYPE html>
<html lang="en">

  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-175204-11"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-175204-11');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Deep NLP Kata - Practice Exercises for Deep Learning and Natural Language Processing</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/scrolling-nav.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Deep NLP Kata</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#about">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section1">Section 1</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section2">Section 2</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section3">Section 3</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section4">Section 4</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section5">Section 5</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section6">Section 6</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section7">Section 7</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section8">Section 8</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section9">Section 9</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#section10">Section 10</a>
            </li>

          </ul>
        </div>
      </div>
    </nav>

    <header class="bg-info text-white">
      <div class="container text-center">
        <h1>Deep NLP Kata</h1>
        <p class="lead">Practice Exercises for Deep Learning and Natural Language Processing</p>
      </div>
    </header>

    <section id="about" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>About</h2>
            <p class="lead">Deep NLP Kata is a set of practice exercises (<i>"Katas"</i>) that help you familiarize with the basic concepts of deep learning for natural language processing. Each exercise is designed to be fun, practical, and bite-sized (i.e., can be completed in less than 30 minutes). </p>
            <p>Those exercises do not depend on the language nor the framework you use. Before starting this exercises, make sure to install the programming language and the framework of your choice and set up the environment.
                We recommend Python + any of major neural network frameworks, such as <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://keras.io/">Keras</a>, <a href="http://pytorch.org/">PyTorch</a>, and <a href="http://caffe.berkeleyvision.org/">Caffe</a>. </p>

            <p>Note that this it <i>not</i> an introductory course for machine learning, deep learning, nor programming. We assume you already know (or know how to Google) the basics of linear algebra, neural networks, and machine learning, as well as how to implement them.</p>

            <p>If you have any feedback / comments / suggestions for this kata, visit the <a href="https://github.com/mhagiwara/deepnlp-kata">Github repo</a>.</p>
          </div>
        </div>
      </div>
    </section>

    <section id="section1">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 1. Linear Algebra Basics</h2>
            <p class="lead">These exercises help you familiarize yourself with the basic building blocks of neural network computation, such as vectors, matrices, and operations.</p>
            <ul>
              <li>
                  <b>Kata 1</b>
                  <p>Calcualte \( 2 + 3 \) using your framework, i.e., <i>not</i> just using Python operations but using e.g., <code>tf.Tensor</code> and <code>torch.Tensor</code>.</p>
              </li>

              <li>
                  <b>Kata 2</b>
                  <p>Caluclate the inner product of two vectors: \( v_1 = [1 \ 3 \ -5] \) and \( v_2 = [4 \ -2 \ -1] \).</p>
              </li>
              <li>
                  <b>Kata 3</b>
                  <p>Calucalte the product of two matrices:
                      $$ m_1 = \begin{bmatrix} -1 & 2 & 3 \\ 4 & -2 & 0 \end{bmatrix} $$ and
                      $$ m_2 = \begin{bmatrix} -2 & -3 \\ 4 & 1 \\ 0 & 1 \end{bmatrix}. $$</p>
              </li>
              <li>
                  <b>Kata 4</b>
                  <p>Consider this linear model: \( y = xw + b \), where \( w = [2 \ -1]^T \) and \( b = 1 \).</p>
                  <p>What is the value of \( y \) when \(x = [1 \ 1], [0 \ 1], [1 \ 0] \), respectively?</p>
              </li>

              <li>
                  <b>Kata 5</b>
                  <p>Consider the linear model \( y = xw + b \), where the values of \( w \) and \( b \) are unknown.</p>
                  <p>When the values of \( x \) are \(x = [1 \ 1], [0 \ 1], [1 \ 0] \), the values of \( y \) are 2, 0, 3, respectively. Solve for \( w \) and \( b \).</p>
                  <p>First, compute the <a href="https://en.wikipedia.org/wiki/Design_matrix">design matrix</a>:
                      $$ X = \begin{bmatrix}
                        1 & x^1_1 & x^1_2  \\
                        1 & x^2_1 & x^2_2  \\
                        1 & x^3_1 & x^3_2
                        \end{bmatrix}, $$
                  where \( x^i_j \) is the j-th dimension of i-th training instance. Then, the solution for \( b \) and \( w \) is given by:
                      $$
                        (X^T X)^{-1} X^T y
                      $$
                </p>
              </li>

              <li>
                  <b>Kata 6</b>
                  <p>Compute the mean squared loss between two vectors: \(y_1 = [1 \ 1 \ 1]\) and \(y_2 = [2 \ 0 \ 3]\).<p>
              </li>

              <li>
                  <b>Kata 7</b>
                  <p>Consider the linear model \( y = xw + b \), where \( w = [0 \ 0]^T \) and \( b = 0 \).
                    For input \(x = [1 \ 1], [0 \ 1], [1 \ 0] \), compute the output (\( y \)) of the model. What is the mean squared loss between \( y \) and desired output \( y = [2 \ 0 \ 3] \)?</p>
                  <p>Also, compute the gradient of the loss w.r.t \( w \) and \( b \).<p>
              </li>

              <li>
                  <b>Kata 8</b>
                  <p>Consider the logistic regression model \( y = \sigma(xw + b) \) where \( w = [0 \ 0]^T \), \( b = 1 \), and \( \sigma \) is a <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>.</p>
                  <p>What are the values of \( y \) for input \(x = [1 \ 1], [0 \ 1], [1 \ 0] \), respectively?</p>
              </li>

              <li>
                  <b>Kata 9</b>
                  <p>Compute the cross entropy loss between the output of Kata 8 and desied output \( \hat{y} = [1 \ 0 \ 0] \).</p>
              </li>

              <li>
                  <b>Kata 10</b>
                  <p>Compute the gradient of the cross entropy loss from Kata 9 w.r.t \( w \) and \( b \).</b>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section id="section2" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 2. Feed-forward Neural Networks</h2>
            <p class="lead">In this section, we'll start building feed-forward neural networks using the building blocks we learned previously.</p>
            <ul>
              <li>
                  <b>Kata 11</b>
                  <p>Let's generate some fake data for regression. Let \( \hat{w} = [2 \ -1] \) and \( \hat{b} = 1 \). These are the parameters you want your neural network to learn. Going forward, pretend you don't know the "true" values of these parameters.<p>
                  <p>First, generate a number (e.g., 100) of two dimensional vectors whose values are drawn randomly from a standard normal distribution (Gaussian distribution with \( \mu = 0 \) and \(\sigma = 1\)). Let this be \( \mathbf{x} \).
                     Second, generate the desired output \( \mathbf{\hat y} \) s.t. \( \mathbf{\hat y} = \mathbf{x} \hat{w} + \hat{b} \).</p>
              </li>

              <li>
                  <b>Kata 12</b>
                  <p>Build a feed-forward neural network that has one densely connected linear layer. It takes \( \mathbf{x} \) as input and output \( \mathbf{y} = \mathbf{x} w + b \). Initialize \( w = [0 \ 0] \) and \( b = 0 \). What does \( \mathbf{y} \) look like?</p>

                  <p>Optionally, you can mini-batch the input \( \mathbf{x} \).</p>
              </li>

              <li>
                  <b>Kata 13</b>
                  <p>Compute the mean squared loss between \( \mathbf{y} \) and \( \mathbf{\hat y} \).</p>
              </li>

              <li>
                  <b>Kata 14</b>
                  <p>Back-propagate the gradient of the loss function and update the model parameters (connection weights and bias of the linear layer) so that the loss is minimized. Iterate this over the training data (i.e., \( \mathbf{x} \) and \( \mathbf{\hat y} \)) multiple times.
                     Log the values of the loss function. If your model and training pipeline is working correctly, those values should decrease as you iterate.</p>
              </li>

              <li>
                  <b>Kata 15</b>
                  <p>Build another feed-forward neural network that has one two-dimensional "hidden" layer. In other words, this model has two sets of connections:
                      <ul>
                          <li>input(2) &rarr;(linear)&rarr; hidden(2) and </li>
                          <li>hidden(2)&rarr;(linear)&rarr;output(1),</li>
                      </ul>
                    where the numbers in ( ) are dimensions of each layer.</p>
              </li>

              <li>
                <b>Kata 16</b>
                <p>As you did in Kata 14, train the model you just created in Kata 15 by back-propagating the loss and updating the model parameters.</p>
                <p>What does the loss decrease look like in comparison to the previous Kata?</p>
              </li>

              <li>
                <b>Kata 17</b>
                <p>Add ReLU (Rectified Linear Unit) as the activation function (non-linearity) to the hidden layer, and train the model.</p>
                <p>Also, try the Tanh and Sigmoid functions. Which activation functions perform better in terms of reducing the loss?</p>
              </li>

              <li>
                <b>Kata 18</b>
                <p>Now, let's turn to a classification problem and generate some fake data for it. Let \( \hat{w} = [2 \ -1] \) and \( \hat{b} = 1 \).</p>
                <p>First, as we did in Kata 11, let's generate a bunch (maybe 100) of two-dimensional vectors from a standard Gaussian distribution. Let this be \( \mathbf{x} \).</p>
                <p>Second, let \( \mathbf{p} = \sigma(\mathbf{x} \hat{w} + \hat{b}) \), where \( \sigma() \) is the element-wise sigmoid function.</p>
                <p>Finally, generate \( \mathbf{\hat y} \) s.t. \( y_i = 1 \) if \( p_i > 0.5\) and 0 otherwise, where \( y_i \) and \( p_i \) are the i-th element of \( \mathbf{\hat y} \) and \( \mathbf{p} \), respectively.</p>
              </li>

              <li>
                <b>Kata 19</b>
                <p>Create a feed-forward neural network that has one two-dimensional hidden layer with the Tanh activation function, as you did in Kata 17.</p>
                <p>Add a sigmoid activation function on top of the output layer, so that the network can output a probability between 0 and 1 instead a real value.</p>
              </li>

              <li>
                <b>Kata 20</b>
                <p>Define a <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy loss</a> between the output of the network \( \mathbf{y} \) and the target \( \mathbf{\hat y} \).</p>
                <p>Back-propagate the gradient of the loss function and update the model parameters.</p>
              </li>
          </div>
        </div>
      </div>
    </section>

    <section id="section3">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 3. Word Embeddings</h2>
            <p class="lead">In this section, we'll be building so-called word2vec models, namely skip-gram and CBOW (continous bag-of-words) models. In terms of architecture, they are very simple feed-forward neural networks.</p>
              <ul>
                <li>
                  <b>Kata 21</b>
                  <p>First of all, let's prepare the data set for the word2vec models. Go to <a href="http://mattmahoney.net/dc/textdata.html">Matt Mahoney's page</a>, find and download <a href="http://mattmahoney.net/dc/text8.zip">text8.zip</a>, and unzip it.
                     This file, consisting of 100M-character excerpts from cleaned English Wikipedia, is often used for testing word2vec algorithms.</p>
                  <p>Second, read the text, split into words by whitespace, and convert words into word indices. In doing so, you want to limit the size of the vocabulary to \( N \) (say, the most frequent 10,000 words).
                     You can do this in the following fashion:</p>
                  <ol>
                    <li>Go through the text (list of words) and count the word occurrences, i.e., number of times each word appears in the text. Python's <code>Counter</code> comes in handy here.</li>
                    <li>Sort the list of unique words by their occurrences, and retain the top \( N - 1 \) most frequently occurring words. (Why \( N - 1 \) not \( N \)? See below.)</li>
                    <li>Assign an integer index from 1 to \( N - 1\) for each unique word in this word list. Now you created a look-up table that converts words to their indices.</li>
                    <li>Go through the text again while looking up in the word-index table.</li>
                    <li>Since the table only contains top-\(N-1\) words, you'll encounter words that are not in the list. Replace such words with a special token <code>UNK</code>. Assign index=0 to <code>UNK</code>, that is, those "out-of-vocabulary" words are assigned an index of 0.</li>
                  </ol>
                  <p>If you do this correctly, you'll have created a long list of word indices, which are integers ranging from 0 to \( N - 1 \). </p>
                </li>

                <li>
                  <b>Kata 22</b>
                  <p>The skip-gram model is trained so that the neural network can solve this "fake task": given a word in text, it predicts what types of words tend to appear in its context. Create the training data for this task as follows:</p>
                  <ol>
                    <li>For each word (let this be \( w_t \)) in the data set, define its context by taking \( c \) words before and \( c \) words after. \( c \) here is called window size.</li>
                    <li>Pick \( k \) (usually \( k \) = 2) words randomly from this "window", and let it be \( w_{t+j} \), where \( j \) is the integer offset from \( w_t \). </li>
                    <li>Now, pairs \( (w_t, w_{t+j}) \) for each \( j \) you picked is the training data.</li>
                  </ol>
                  <p>In machine learning, we usually group such training pairs into <i>minibatches</i>.
                     Continue this process for different values of \( t \) and \( j \) until you have \( b \) such pairs. Group them into a minibatch. </p>
                </li>

                <li>
                  <b>Kata 23</b>
                  <p>The key to word2vec models is <i>word embeddings</i>, which are semantic representations of words through dense, real-valued vectors. Word2vec models convert input/output words to embeddings and use them for prediction.</p>
                  <p>Conversion from words to embeddings is usually done via an <i>embedding matrix</i>, which works as a look-up table from word IDs to their corresponding embedding vectors.
                     First, define an embedding matrix in your programming language/framework. Second, represent a word via a one-hot vector, which is a sparse, integer vector where i-th element (i = ID of the word) is 1 and everything else is 0.
                     Finally, the corresponding word vector is obtained by multiplying this one-hot vector with the embedding matrix. (Note that most modern neural network frameworks have a built-in implementation for this whole pipeline).</p>
                </li>

                <li>
                  <b>Kata 24</b>
                  <p>Implement the skip-gram model. First, using the embedding matrix above, obtain the input word vector. Let this be \( v_{w_I} \).
                     Second, using another embedding matrix (but with the same size and shape), obtain the output word vector \( v'_{w_O} \). Take the inner product between those two. Finally, normalize this by considering all words in the vocabulary using softmax:</p>
                     $$
                       p(w_O | w_I) = \frac{\exp({v'_{w_O}}^T v_{w_I})}{\sum_{w \in V} \exp({v'_w}^T v_{w_I})) },
                     $$
                     where \( V \) is the vocabulary. The loss function is the sum of the logarithm of the probability above over all training pairs:
                     $$
                       \sum_{(w_O, w_I) \in X} \log P(w_O | w_I).
                     $$
                     <p>Train your skip-gram model by maximizing the log probability above.</p>
                </li>

                <li>
                  <b>Kata 25</b>
                  <p>After training the skip-gram model, as a by-product you end up with the "optimized" embedding matrix, whose rows correspond to what you want as word vectors.</p>
                  <p>Pick one word arbitrarily from the vocabulary (say, "dog"), and compute the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> between this word vector and every other word vector in the vocabulary, and
                     sort the words by this similarity. Do you see similar words to the one you picked?</p>
                </li>
              </ul>
            <p class="lead"></p>
          </div>
        </div>
      </div>
    </section>

    <section id="section4">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 4. Language Modeling</h2>
            <p class="lead"></p>
          </div>
        </div>
      </div>
    </section>

    <section id="section5">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 5. Text Generation</h2>
            <p class="lead"></p>
          </div>
        </div>
      </div>
    </section>

    <section id="section6">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 6. Text Classification</h2>
            <p class="lead"></p>
          </div>
        </div>
      </div>
    </section>

    <section id="section7">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 7. Sequence-to-Sequence Models</h2>
            <p class="lead"></p>
          </div>
        </div>
      </div>
    </section>

    <section id="section8">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 8. Attention</h2>
            <p class="lead"></p>
          </div>
        </div>
      </div>
    </section>


    <section id="section9">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 9. Convolutional Neural Networks</h2>
            <p class="lead"></p>
          </div>
        </div>
      </div>
    </section>

    <section id="section10">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h2>Section 10. (TBD)</h2>
            <p class="lead"></p>
          </div>
        </div>
      </div>
    </section>

    <!-- Footer -->
    <footer class="py-5 bg-dark">
      <div class="container">
        <p class="m-0 text-center text-white">Copyright &copy; <a href="http://masatohagiwara.net/">Masato Hagiwara</a> 2018.<br />
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a> This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>
      </div>
      <!-- /.container -->
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>

    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
  </body>

</html>
